{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing,tree,metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "datas_train_pd = pd.read_csv('~/midterm/MT_Train.csv',header=0)\n",
    "datas_test_sampled = pd.read_csv('~/midterm/MT_Test.csv',usecols=[0])\n",
    "datas_test_pd = pd.read_csv('~/midterm/MT_Test.csv',header=0,usecols=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "datas_train_np = datas_train_pd.values\n",
    "datas_test_np = datas_test_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_num_list = [1,2,3,4,5,6,7,8,9,14,20]\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in non_num_list:\n",
    "#     le.fit(datas_train_np[0:,i])\n",
    "    datas_train_np[0:,i] = le.fit_transform(datas_train_np[0:,i])\n",
    "    if i!=20:\n",
    "        datas_test_np[0:,i] = le.fit_transform(datas_test_np[0:,i])\n",
    "# print datas_train_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_train_x = datas_train_np[0:,0:20].tolist()\n",
    "datas_train_y = datas_train_np[0:,20].tolist()\n",
    "datas_test_x = datas_test_np[0:,0:20].tolist()\n",
    "# print datas_test_x\n",
    "# print datas_train_x\n",
    "# print datas_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=120,\n",
       "              presort='auto', random_state=15, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(datas_train_x,datas_train_y,test_size=0.4,random_state=2)\n",
    "# GDBT方法\n",
    "gbm0 = GradientBoostingClassifier(random_state=15,n_estimators=120,max_depth=5)\n",
    "gbm0.fit(X_train,y_train)\n",
    "\n",
    "# print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94      3493\n",
      "          1       0.72      0.68      0.70       739\n",
      "\n",
      "avg / total       0.90      0.90      0.90      4232\n",
      "\n",
      "0.896555679078\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbm0.predict(X_test)\n",
    "# y_predprob = gbm0.predict_proba(X)[:,1]\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print metrics.f1_score(y_test,y_pred,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test1 = {'n_estimators':range(20,201,5)}\n",
    "# gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "#                                   min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10), \n",
    "#                        param_grid = param_test1, scoring='roc_auc',iid=False,cv=5)\n",
    "# gsearch1.fit(X_train,y_train)\n",
    "# print gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test2 = {'max_depth':range(13,33,5), 'min_samples_split':range(10,511,100)}\n",
    "# gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=120, min_samples_leaf=20, \n",
    "#       max_features='sqrt', subsample=0.8, random_state=10), \n",
    "#    param_grid = param_test2, scoring='roc_auc',iid=False, cv=5)\n",
    "# gsearch2.fit(X_train,y_train)\n",
    "# gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm0 = GradientBoostingClassifier(random_state=15,n_estimators=500)\n",
    "# gbm1.fit(datas_train_x,datas_train_y)\n",
    "y_pred = gbm0.predict(datas_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datas_test_sampled\n",
    "le.fit(['no','yes'])\n",
    "out=le.inverse_transform(y_pred)\n",
    "# print datas_test_sampled.shape\n",
    "# print out.shape,y_pred.shape\n",
    "submission = pd.DataFrame({\n",
    "        \"SampleId\": datas_test_sampled[\"SampleId\"],\n",
    "        \"y\": out\n",
    "    })\n",
    "submission.to_csv('~/midterm/outcome.csv',index=False,sep=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out2 = pd.read_csv('/home/codezhong/midterm/outcome_3.csv',header=0)\n",
    "# out2=out2.values\n",
    "# out3 = pd.read_csv('/home/codezhong/midterm/outcome_11.csv',header=0)\n",
    "# out3=out3.values\n",
    "# a = 0\n",
    "# b=0\n",
    "# for i in range(out2.shape[0]):\n",
    "#     if (out2[i,1]==out3[i,1]):\n",
    "#         a=a+1;\n",
    "#     b=b+1\n",
    "# print a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉驗證結果:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.93      0.92      1738\n",
      "          1       0.66      0.63      0.65       378\n",
      "\n",
      "avg / total       0.87      0.88      0.88      2116\n",
      "\n",
      "0.875261721796\n"
     ]
    }
   ],
   "source": [
    "# 决策树方法\n",
    "# clf = tree.DecisionTreeClassifier(criterion=\"entropy\",splitter=\"best\",\n",
    "#                                  max_depth=20,min_samples_split=5,\n",
    "#                                      min_samples_leaf=10)\n",
    "# # clf = clf.fit(datas_train_x,datas_train_y)\n",
    "# # y_pred = clf.predict(datas_test_x)\n",
    "# # print y_pred.shape[0]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(datas_train_x, datas_train_y, test_size=0.2,random_state=i)\n",
    "# clf = clf.fit(X_train,y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print \"交叉驗證結果:\"\n",
    "# print metrics.classification_report(y_test, y_pred)\n",
    "# print metrics.f1_score(y_test, y_pred,average = \"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉驗證結果:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.94      1738\n",
      "          1       0.71      0.69      0.70       378\n",
      "\n",
      "avg / total       0.89      0.89      0.89      2116\n",
      "\n",
      "0.892746179453\n",
      "[ 0.03890711  0.01807674  0.0052707   0.0147764   0.00190645  0.00678973\n",
      "  0.00337381  0.00492484  0.01774543  0.01955005  0.40304759  0.01119533\n",
      "  0.04083445  0.0025685   0.0128771   0.00586099  0.01547715  0.02995536\n",
      "  0.13421257  0.21264968]\n"
     ]
    }
   ],
   "source": [
    "# 随机森林方法\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# model = RandomForestClassifier(max_depth=12,min_samples_leaf=10, min_samples_split=5,n_estimators=30,max_features=12)\n",
    "# model.fit(datas_train_x , datas_train_y) # 创建一个随机森林\n",
    "# y_pred=model.predict(datas_test_x) # 对新的样本Z做预测\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(datas_train_x, datas_train_y, test_size=0.2,random_state=i)\n",
    "# mode = model.fit(X_train,y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# print \"交叉驗證結果:\"\n",
    "# print metrics.classification_report(y_test, y_pred)\n",
    "# print metrics.f1_score(y_test,y_pred,average=\"weighted\")\n",
    "# print model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.97      0.92      1738\n",
      "          1       0.72      0.38      0.50       378\n",
      "\n",
      "avg / total       0.85      0.86      0.84      2116\n",
      "\n",
      "0.844512257356\n"
     ]
    }
   ],
   "source": [
    "# 神经网络——多层感知机方法\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(10,20,10,10),max_iter=500,solver='lbfgs')\n",
    "# mlp.fit(X_train,y_train)\n",
    "# predictions = mlp.predict(X_test)\n",
    "# print metrics.classification_report(y_test, predictions)\n",
    "# print metrics.f1_score(y_test,predictions,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
